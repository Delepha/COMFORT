# Automated Conformance Testing for JavaScript Engines via Deep Compiler Fuzzing: Artifact

This directory contains the supporting artifact for our paper (COMFORT) on PLDI 2021 paper on Javascript Conformance Testing. It contains reduced-size data sets for evaluating our GPT-2 based test program generator, test case mutation and reduction, and differential tester. The full dataset is quite large (>100 GB uncompressed), and we are working on finding a method for sharing it with the community. The idea is that this directory contains minimal working examples which can be evaluated in a reasonable amount of time. All of our code and data will be open-sourced upon publication and has been developed with extensibility as a primary goal.

# Overview of Results
As stated in Section 5 of the PLDI submission, we have started experimenting with and refining our tool since May 2019. At the time of our PLDI 2021 submission (21 Nov 2020), we have indentified 158 unique bugs, of which 129 have been verified, and 115 have been fixed by the developers. Furthermore, 21 of the COMFORT generated test cases were added into Test-262, the official Javascript conformance test suite.

A list of bugs discovered by COMFORT can be found at the [Bug List](#bug-list) section. We also list the COMFORT-generated test cases that were accepted by the [Test-262 test suite](https://github.com/tc39/test262) in the [Test-262 accepted test case section](#test-262). In addition, we also list the [newly discovered bugs](#newly-discovered) found by COMFORT after the PLDI submission.

## Bug List <span id = "bug-list">

Here we provide a [list of bugs](./Bug-List.md) exposed by COMFORT-generated test cases. 

# Getting Start Guide
For convenience, we have provided a pre-configured live server with a Python Jupyter Notebook to work through our techiques (Please see the ReadMe document on the AE submission website on how to access the Notebook). The notebook provides instructions on how to test our techniques on a small-scale dataset. 

Note that we do not log IP addresses or other accessing information, but if you have concerns on this, you could use a [Tor Browser](https://www.torproject.org/download/) to access our demo website. 

# Step-by-Step Instructions

## ★ Docker Image
We prepare our artifact within a Docker image to run "out of the box". The Docker image can be downloaded from [here](https://drive.google.com/drive/folders/1JkS2S4GOCPdicQsbDeqlkzXO4tZ-2Iyg?usp=sharing). 

After downloading the docker image, using the following commands to load the docker image on the host machine:
```
unzip 53.zip
cd 53
docker load -i 53.tar
```

## ★ Configure the GPU running environment
If you wish to use an NVIDIA GPU on the host machine to run the AE, please follow the instructions below to setup the GPU execution environment:

> - Copy [this bash script](../data/nvidia-container-runtime-script.sh) to the user directory with sudo permission. Then run the following command:
> 
>     ```bash nvidia-container-runtime-script.sh```
>      
>      Note that this step may break the existing docker environment.
>  
> - Next, test if the GPU running environment is successfully configured:
>  
>   ```docker run --help | grep -i gpus  ```
>   
>     You should be able to see the the GPU information if it is successfully configured.
>  
> - Finally, run the following command to import the docker container:
>  
>   ```docker run -itd --name comfort --gpus all pldi2021:comfort /bin/bash  ```

## ★ Setup
After importing the docker container, make sure you run ``` source /root/.bash_profile``` to setup the environmental variables, before using any of the following scripts. 


## ★ Artifact Contents
The Docker image contains the following scripts for evaluation. 

 * [01_evaluate_generator.py](https://github.com/NWU-NISL-Fuzzing/COMFORT/blob/main/artifact_evaluation/src/01_evaluate_generator.py): A demonstration of training a GPT-2 model to generate JS programs. 
 * [02_evaluate_mutator.py](https://github.com/NWU-NISL-Fuzzing/COMFORT/blob/main/artifact_evaluation/src/02_evaluate_mutator.py): A demonstration of test program mutation. This demonstrate how we mutate the test JS programs generated by the previous demonstration.
 * [03_evaluate_harness.py](https://github.com/NWU-NISL-Fuzzing/COMFORT/blob/main/artifact_evaluation/src/03_evaluate_harness.py): A demonstration of our differential testing approach. The results from the prior steps are combined with the test cases generated by our full-scale evluation used in the paper to perform differential testing.

## ★ Train the JS program generator <br id="generator">
(*approximate runtime: ~1 hour for using a GPU*)

* Evaluate GPT-2 program synthesizer by running the following command (set ```--multi_gpu=0``` for using CPU for training):

``` python /root/src/01_evaluate_generator.py --mode=finetune  --multi_gpu=1 ```

The program uses a small JS corpus of 2,000 JS programs randomly selected from our entire training corpus to refine a scale-downed, pre-trained GPT-2 model (that was trained on natural language texts) on the JS corpus.

We have reduced the size of the corpus so that it takes around 5 hours to train on a multi-core CPU (~1 hour on a GPU). For our paper, we trained our model on more data (140,000 JS programs rather than 2,000) for longer (~150,000 iterations rather than 1,000). *As such, the quality of output of this model is lower, which is likely to produce shorter and fewer syntactically correct programs*. 

Training the model can be interrupted and resumed at any time. Once trained, the model does not need to be re-trained. 


### Evaluation of Our JS Program Generator 

#### ☆ Program generation using the trained model
(*approximate runtime: 15 minutes for using a GPU*)

* To use the [trained model](#generator) to generate the test programs, run the following command (set ```--multi_gpu=0``` for using the CPU for inference, approximate 1 hour): 

``` python /root/src/01_evaluate_generator.py --mode=generate --use_nisl_model=0 --multi_gpu=1 --nsamples=512 ```

The  ```--nsamples``` parameter controls how many test programs to generate. **Note that the value of ```--nsamples``` should be a multiply of the default bach size of 16 (e.g., 16, 32, 64, etc.)**.

#### ☆ Program generation using our pre-trained model (*Optional*)
We also provided the full-trained GPT-2 JS program generator used by our paper. Our pre-trained model is stored in ``` /root/src/generate_model/models/nisl_model```.
 You can use the following command to generate about 512 test programs (defined by nsamples) where each test program contains around 4 JS APIs (which lead to ~4 * 512 = 2,000 test cases). 

* You can run the test on the GPU by using the following command (set ```--multi_gpu=0``` to run on the CPU)：

``` python /root/src/01_evaluate_generator.py --mode=generate --use_nisl_model=1 --multi_gpu=1 --nsamples=512 ```

All generated test cases are written to directory ```/root/data/generated_data/complete_testcases/```. 

#### ☆ Evaluation of the code coverage 
(*approximate runtime: 15 minutes*)

* You can use the following command to compute the percentage of the generated test programs passed [JSHint](https://jshint.com/) (a static JS syntax chcker), and the coverage repored by [Istanbul](https://istanbul.js.org/). 

``` python /root/src/05_coverage_calculate.py --fuzzer=comfort --reporter_dir=/root/data/codeCoverage/coverageReporters ```

You can change the value of the parameter ```--fuzzer``` to be  `codealchemist, deepsmith, die, fuzzilli or montage`, to calculate the code coverage of other fuzzers.

* Note that we randomly selected ~1000 test cases for each fuzzer. All the test cases (10,000) for each fuzzer used in our paper are stored in  ``` /root/data/codeCoverage/totalFiles```. You can also use all the test cases using the following command for longer evaluation (12+ hours):

``` python /root/src/05_coverage_calculate.py --coverage_files=/root/data/codeCoverage/totalFiles/comfort_generate --reporter_dir=/root/data/codeCoverage/coverageReporters ```

This data corresponds to Figure 8. Note that since the test programs are randonmly chosen, the numbers may be slightly different from the ones reported in the paper. 

## ★ Demonstration of test program mutation
(*approximate runtime: 5 minutes*)

* Evaluate our ECMAScript-guided test data generator by running the following command:

```
python /root/src/02_evaluate_mutator.py --input_path=/root/data/generated_data/complete_testcases --save_path=/root/data/mutation_result
```

**Note that our tool can only mutate test programs with a JS APIs. If the test program does not contain a JS API, it will yield an eror meassge `This test case fails to be mutated as it does not contain any API.`** When all test cases are mutated, you could see the number of test cases that are mutated successfully.


## ★ Demonstration of Differential testing
(*approximate runtime: 10 minutes*)

* Evaluate our differential fuzzer on a JS test bed by running the following command:

```python /root/src/03_evaluate_harness.py --testsuite=/root/data/mutation_result/ --clear_classifier=False```
